{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Twitter data collection and analysis using R\n",
    "### A case study for COVID keywords co-occurrence on Saturday evening, May 30, 2020, in Paris "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this jupyter notebook we will show how to collect data from Twitter using the standard Twitter API and will perform some analysis using using `rtweet`, and specifically, geocoding and co-occurrence matrices.\n",
    "\n",
    "The data collected corresponds to the activity on Twitter within a radius of 50 kilometers around Paris, afternoon of Saturday, May 30, 2020, between 18:45:00 and 22:45:00, coinciding with the COVID lockdown ease in France. This provided an acceptable testing ground for this type of quantitative analyses. We collected a total of 128.434 unique tweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Job schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We used an Unix scheduler called `cron` to setup repeated tasks. In our case, the task involved executing our R script each 20 min. This can be easily configured from the Linux bash shell as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. Enter the editor using `crontab -e`\n",
    "2. Whithin the cron tab, type a line contaning instructions for the job, path, executable Rscript and file.R. In our case, we ran the task each 20 min to avoid problems with the API rate limit. Thus, we add a line as follows:\n",
    "`*/20 * * * * cd /path/to/file; Rscript collect.R`\n",
    "3. Save changes by doing `Ctr+x`\n",
    "4. Start cron service: `sudo service cron start`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Collecting Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The folloging code contains:\n",
    "\n",
    "1. A shebang that invokes an interpreter for executing the script: `#!/usr/bin/env Rscript`.\n",
    "2. Api name and keys.\n",
    "2. The function `search_tweets`, which is used to define the keywords (for this, you need a [Twitter API account](https://developer.twitter.com/en)), language, retryonratelimit, geocode and number of tweets to be retrieved.\n",
    "3. Specification of directories.\n",
    "4. Code to save file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env Rscript\n",
    "#\n",
    "#\n",
    "\n",
    "library(rtweet)\n",
    "\n",
    "# Twitter API\n",
    "create_token(\n",
    "  app = \"your_api_name\",\n",
    "  consumer_key = \"xxxxxxx\",\n",
    "  consumer_secret = \"xxxxxxx\",\n",
    "  access_token = \"xxxxxxx\",\n",
    "  access_secret = \"xxxxxxx\"\n",
    ")\n",
    "\n",
    "# 50KM radius around Paris\n",
    "geocode <-  \"48.8534,2.3486,50km\"\n",
    "\n",
    "#Search specifications\n",
    "newTweets <- search_tweets(q = \"covid\", \n",
    "                    lang = \"fr\",\n",
    "                    retryonratelimit = FALSE, \n",
    "                    geocode = geocode,\n",
    "                    include_rts = FALSE, \n",
    "                    n = 10000)\n",
    "\n",
    "\n",
    "# Specify directory\n",
    "dirPath <- \"/path/to/file/\"\n",
    "\n",
    "# Create directory for storage\n",
    "if(!dir.exists(paste0(dirPath, \"tweets/\"))){\n",
    "  dir.create(\"tweets/\")\n",
    "}\n",
    "\n",
    "# Write csv with date (I am using Sys.time for this search. If you want to space the search \n",
    "# in days you can save the files using Sys.Date() instead)\n",
    "save_as_csv(newTweets, paste0(dirPath,\"tweets/\",format(Sys.time(),\"%d_%m_%Y_%H_%M_%S\"), \".csv\"),\n",
    "            prepend_ids = TRUE, na = \"\",\n",
    "            fileEncoding = \"UTF-8\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  Île-de-France geographical distribution of the harvested tweets for COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(rtweet)\n",
    "library(tidyverse)\n",
    "library(reshape2)\n",
    "library(ggplot2)\n",
    "library(ggridges)\n",
    "library(lubridate)\n",
    "library(rtweet)\n",
    "library(maps)\n",
    "library(quanteda)\n",
    "\n",
    "# List all files\n",
    "allFiles <- paste0(\"tweets/\", list.files(\"tweets/\"))\n",
    "\n",
    "# Read all csv files in the folder and create a list of dataframes\n",
    "mergeTweets <- lapply(allFiles , read.csv)\n",
    "\n",
    "# Combine each dataframe in the list into a single dataframe\n",
    "allTweets <- do.call(\"rbind\", mergeTweets)\n",
    "\n",
    "# Write CSV\n",
    "write_as_csv(allTweets, file_name = \"gotTwitter_2.csv\")\n",
    "\n",
    "# Read final dataset\n",
    "allTweets <- read_twitter_csv(\"/your_path/gotTwitter_2.csv\", unflatten = T)\n",
    "\n",
    "# Convert UTC to EDT\n",
    "allTweets %<>% dplyr::mutate(created_at = as_datetime(created_at, tz = \"UTC\")) %>%\n",
    "  dplyr::mutate(created_at = with_tz(created_at, tzone = \"Europe/Paris\"))\n",
    "\n",
    "# Produce lat and lng coordinates\n",
    "allTweets <- lat_lng(allTweets)\n",
    "# Plot\n",
    "par(mar = rep(1, 4))\n",
    "#map(\"france\", lwd = .25)\n",
    "#Importation du package\n",
    "\n",
    "library(raster)\n",
    "\n",
    "#Découpage en département\n",
    "FranceFormes <- getData(name=\"GADM\", country=\"FRA\", level=3)\n",
    "\n",
    "#Sélection que d'une région: Île de France\n",
    "#Pour trouver les noms des variables de l'objet FranceFormes\n",
    "names(FranceFormes)\n",
    "\n",
    "#Pour trouve les les noms des régions de France\n",
    "FranceFormes$NAME_1\n",
    "\n",
    "#Sélection du département\n",
    "FranceFormesParis <- subset(FranceFormes, NAME_1==\"Île-de-France\")\n",
    "plot(FranceFormesParis, main= \"Île-de-France geographical distribution of the harvested tweets for COVID\")\n",
    "\n",
    "# plot lat and lng points onto state map\n",
    "with(allTweets, points(lng, lat,\n",
    "                       pch = 16, cex = .25,\n",
    "                       col = rgb(.8, .2, 0, .2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Paris.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Analysys of co-ocurrence of keywords related to COVID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will use tokenise the tweets, that is, we will break the tweets into individual linguistic units (tokens that can comprise single or multiple words, such as n-grams). Tokenisation will involve cleaning the tweets to get rid of irrelevant elements (e.g. separators, punctuation, URLs, etc...). \n",
    "\n",
    "Then we will create a [document-feature matrix (DFM)](https://www.rdocumentation.org/packages/quanteda/versions/2.0.1/topics/dfm). Our `dfmatrix` contained 19.311.849.976 elements. To investigate the co-occurrence of a pair of eleemnts A and B, we follwed the method proposed by de Abreu e Lima (2019). Let $CO_{A,B}$ be the co-occurrence of any pair of elements, then:\n",
    "\n",
    "\\begin{equation}\n",
    "CO_{A,B} = \\sum_{i=1}^{n} 1_{T_i}(\\{A,B\\}) := \\begin{cases} 1, \\text{if} \\{A,B\\} \\subseteq T_i \\\\ 0, \\text{otherwise} \\end{cases} \n",
    "\\end{equation}\n",
    "\n",
    "where $T_i$  is the $ith$ tweet, and a mathematical set with as many elements as tokens. The summation over the function $1_{T_i}(\\{A,B\\})$ counts tweets where both A and B are mentioned, which can be used as a reliable metric to account for the association between two tokens.\n",
    "\n",
    "For the current test, we used a number of names of political figures, along with relevant keywords used in the political arena at the moment. This way, our matrix was a 31 x 31 with all the counts in our `dfmatrix`. The minimum frequency was set to 0.1.\n",
    "\n",
    "For illustration purposes we add the visualization of the feature co-occurrence matrix as a network of interactions between terms. The minimum frequency was set to 0.1. Edges width represent relative frequency of connections between terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize words\n",
    "tkn <- tokens(allTweets$text,\n",
    "              remove_separators = T,\n",
    "              remove_symbols = T,\n",
    "              remove_punct = T,\n",
    "              remove_url = T,\n",
    "              split_hyphens = T,\n",
    "              remove_numbers = T) %>%\n",
    "  tokens_ngrams(n = 1:2)\n",
    "\n",
    "dfmatrix <- dfm(tkn, tolower = T,\n",
    "              remove = stopwords(\"french\"))\n",
    "\n",
    "gotChars <- c(\"Trump\",\"Johnson\",\"Macron\",\"Merkel\",\"Hidalgo\",\"Dati\", \"Mélenchon\", \"Le Pen\",\n",
    "              \"Buzyn\", \"Belliard\", \"Bournazel\", \"Federbusch\", \"Gantzer\", \"Simonnet\", \"Griveaux\", \"Benjamin\",\n",
    "              \"covid\",\"climate\", \"liberte\", \"egalite\", \"fraternite\", \"coronavirus\",\n",
    "              \"sante\", \"police\", \"bondy\", \"musk\", \"nasa\", \"morts\", \"paris\", \"france\", \"europe\")\n",
    "\n",
    "gotFcm <- dfm_select(dfmatrix, pattern = gotChars) %>%\n",
    "  fcm()\n",
    "\n",
    "textplot_network(gotFcm, min_freq = 0.1,\n",
    "                 edge_alpha = .7,\n",
    "                 edge_size = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Network.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "Dimension statistiques and sociétés. (Consulted 5/31/2020). Retrieved from http://dimension.usherbrooke.ca/dimension/ssrcartes.html\n",
    "\n",
    "de Abreu e Lima, F (2019). poissonisfish: Twitter data analysis in R. \n",
    "From https://poissonisfish.com/2019/10/09/twitter-data-analysis-in-r/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
